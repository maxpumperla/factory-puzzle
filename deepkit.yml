config:
  # Fixed settings (don't touch)
  env_name: factory
  actions: 5
  low: -1
  high: 10

  # Factory settings
  layout: big  # either "big" or "small"
  num_tables: 12
  num_cores: 8
  num_phases: 1
  random_init: true  # if random initialization is False, the factory will reset to the same state after each episode.
  # seed: 1337  # Optional random seed to have full control over initial factory state.

  # Logging & Tracking
  local_mode: false  # if "True", will start ray as single process for easier debugging in your IDE
  deepkit_logging: true  #
  deepkit_account: deepkit # or localhost
  experiment_list: my_experiment

  # RL-specific configuration
  env: MultiAgentFactoryEnv  # "FactoryEnv", "RoundRobinFactoryEnv", "MultiAgentFactoryEnv"
  max_num_steps: 10000 # NOTE: need to set this high enough for the big factory
  masking: true  # whether to use action masking
  algorithm: PPO  # Choose from PPO, DQN, MARWIL
  use_offline_data: false  # Use previously generated offline data (don't use for now, experimental)
  offline_data_ratio: 0.5
  num_samples: 4  # Ray rllib's "num_samples" extracted for convenience
  multi_policy: false  # Using multiple policies or not. This only works for "MultiAgentEnv"
  fcnet_hiddens: [512, 512]  # [1024, 1024]

  # Observation selection
  ## Agent & core obs
  obs_agent_id: false
  obs_agent_coordinates: false
  obs_agent_has_core: false
  obs_agent_core_target_coordinates: false

  ## Neighbour obs
  obs_agent_has_neighbour: false
  obs_agent_free_neighbour: true

  ## One-hot representation obs
  obs_agent_id_one_hot: true
  obs_agent_core_target_one_hot: true
  obs_all_tables_one_hot: true
  obs_all_cores_one_hot: false

  # Reward selection
  rew_collisions:
    value: true
    weight: 1
  rew_found_target:
    value: true
    weight: 30
  rew_avoid_cores:
    value: true
    weight: 1
  rew_punish_slow_tables:
    value: false
    weight: 50
image: python:3.7

build:
- ADD requirements.txt
- pip install -r requirements.txt
- ADD factory/
- ADD setup.py
- python setup.py install

#env:
#  - PYTHONPATH=/job

output:
- DQN
- PPO

ignore:
- DQN
- PPO
- models
- tests
- assets
- venv
- .pytest_cache

title: Factory solver

list: my_experiment

labels:
- rllib
- tensorflow

command: python examples/save_trainer.py
